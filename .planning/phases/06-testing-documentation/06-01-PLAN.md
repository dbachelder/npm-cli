---
phase: 06-testing-documentation
type: execute
---

<objective>
Implement integration testing infrastructure with NPM container, CliRunner tests, and HTTP mocking patterns.

Purpose: Enable reproducible end-to-end testing against real NPM instances without production risk.
Output: Working testcontainers setup, CliRunner integration tests, pytest-httpx mocking patterns, all tests passing.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
~/.claude/get-shit-done/references/tdd.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-testing-documentation/06-RESEARCH.md
@.planning/phases/05-configuration-templates/05-01-SUMMARY.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
@pyproject.toml
@src/npm_cli/__main__.py
@src/npm_cli/api/client.py
@src/npm_cli/cli/proxy.py
@tests/test_templates.py
</context>

<tasks>

<task type="auto" tdd="true">
  <name>Task 1: Add test dependencies and create conftest.py with NPM container fixture</name>
  <files>pyproject.toml, tests/conftest.py, tests/test_integration_setup.py</files>
  <test-first>
    Test: Verify NPM container fixture starts and provides connection info
    Cases: fixture provides NPM_HOST env var, NPM_PORT env var, container is reachable on port 81
  </test-first>
  <action>
    Add testcontainers and pytest-httpx to dev dependencies using `uv add --dev testcontainers pytest-httpx`.

    Create tests/conftest.py with module-scoped npm_container fixture using testcontainers DockerContainer pattern from RESEARCH.md. Use jc21/nginx-proxy-manager:latest image, expose port 81, set DISABLE_IPV6=true environment variable. After container.start(), set os.environ["NPM_HOST"] and os.environ["NPM_PORT"] from container methods. Add time.sleep(10) for NPM initialization (per RESEARCH.md Pitfall #1 - NPM takes 5-10 seconds to initialize). Use request.addfinalizer(cleanup) pattern (not yield) for container.stop() to ensure cleanup even on fixture setup failures (per RESEARCH.md Pitfall #2).

    Create tests/test_integration_setup.py with test_npm_container_starts(npm_container) that verifies environment variables are set and attempts httpx.get() to http://{NPM_HOST}:{NPM_PORT}/api/schema to confirm NPM is responding (this endpoint exists per Phase 1 API-DISCOVERY.md).

    Do NOT use docker-compose fixtures - testcontainers is Python-native with better pytest integration per RESEARCH.md. Do NOT use yield for critical cleanup - use request.addfinalizer() to prevent orphaned containers on setup failures.
  </action>
  <verify>uv run pytest tests/test_integration_setup.py -v passes, docker ps shows container running during test, docker ps shows no orphaned containers after test completion</verify>
  <done>NPM container fixture exists, starts reliably, exports connection info, cleans up properly, integration setup test passes</done>
</task>

<task type="auto" tdd="true">
  <name>Task 2: Implement CliRunner integration tests for key commands</name>
  <files>tests/test_cli_integration.py</files>
  <test-first>
    Test: CliRunner can invoke app commands and capture output
    Cases: version command returns exit_code=0 with version string, proxy list returns exit_code=0 or auth error (expected without credentials)
  </test-first>
  <action>
    Create tests/test_cli_integration.py using Typer's CliRunner pattern from RESEARCH.md. Import CliRunner from typer.testing and app from npm_cli.__main__.

    Implement test_version_command() that invokes runner.invoke(app, ["version"]) and asserts exit_code == 0 and "npm-cli version" in result.output.

    Implement test_proxy_list_no_auth() that invokes runner.invoke(app, ["proxy", "list"]) and verifies graceful handling (either success if no auth required for listing, or clear error message about missing authentication - both are acceptable).

    Implement test_help_output() that invokes runner.invoke(app, ["--help"]) and verifies exit_code == 0 and presence of subcommand names ("proxy", "cert", "config").

    Use CliRunner() NOT subprocess.run - CliRunner is faster with no subprocess overhead per RESEARCH.md Architecture Patterns. Create new CliRunner instance for each test to avoid state caching issues (RESEARCH.md Pitfall #6).
  </action>
  <verify>uv run pytest tests/test_cli_integration.py -v passes with all CLI invocation tests green</verify>
  <done>CliRunner tests exist for version, help, and proxy list commands, all tests pass, no subprocess calls used</done>
</task>

<task type="auto" tdd="true">
  <name>Task 3: Add pytest-httpx mocking patterns for API client tests</name>
  <files>tests/test_api_client_mocked.py</files>
  <test-first>
    Test: httpx_mock fixture can intercept NPM API calls
    Cases: POST /api/tokens returns mocked token, GET /api/nginx/proxy-hosts returns mocked proxy list
  </test-first>
  <action>
    Create tests/test_api_client_mocked.py demonstrating pytest-httpx patterns from RESEARCH.md for NPMClient methods.

    Implement test_authenticate_success(httpx_mock) that uses httpx_mock.add_response() to register POST http://npm:81/api/tokens with json={"token": "fake-jwt-token"}, then calls NPMClient.authenticate() and asserts token == "fake-jwt-token".

    Implement test_list_proxy_hosts_mocked(httpx_mock) that registers GET http://npm:81/api/nginx/proxy-hosts with json=[{"id": 1, "domain_names": ["test.example.com"]}], then calls NPMClient.list_proxy_hosts() and asserts result contains the mocked proxy.

    Implement test_create_proxy_host_validation(httpx_mock) that registers POST http://npm:81/api/nginx/proxy-hosts with status_code=400 and error response, then verifies NPMClient.create_proxy_host() raises appropriate NPMAPIError with response details preserved (per Phase 3 custom exception hierarchy).

    Use pytest-httpx NOT manual mock.patch - pytest-httpx understands httpx patterns better per RESEARCH.md. If httpx_mock doesn't match requests, check exact URL format and consider match_headers=False (RESEARCH.md Pitfall #5).
  </action>
  <verify>uv run pytest tests/test_api_client_mocked.py -v passes with all HTTP mocking tests green</verify>
  <done>pytest-httpx mocking patterns demonstrated for auth, list, create operations, all tests pass, NPMAPIError handling verified</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `uv run pytest` passes all tests (140 existing + new integration tests)
- [ ] `docker ps` shows no orphaned containers after test run
- [ ] testcontainers dependency installed in pyproject.toml dev group
- [ ] pytest-httpx dependency installed in pyproject.toml dev group
- [ ] conftest.py exists with working NPM container fixture
- [ ] Integration tests demonstrate CliRunner, testcontainers, and pytest-httpx patterns
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- testcontainers and pytest-httpx installed
- NPM container fixture works reliably with cleanup
- CliRunner tests cover key commands (version, help, proxy)
- pytest-httpx mocking patterns established for API client
- Test suite remains green (all existing + new tests passing)
  </success_criteria>

<output>
After completion, create `.planning/phases/06-testing-documentation/06-01-SUMMARY.md`:

# Phase 6 Plan 1: Integration Testing Setup Summary

**[Substantive one-liner - what shipped]**

## Accomplishments

- [Key outcome 1]
- [Key outcome 2]

## Files Created/Modified

- `path/to/file.ts` - Description

## Decisions Made

[Key decisions and rationale, or "None"]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Step

Ready for 06-02-PLAN.md (Documentation & Completion)
</output>
